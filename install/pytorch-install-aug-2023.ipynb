{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/jeffheaton/app_deep_learning/blob/main/install/pytorch-install-aug-2023.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jeffheaton/app_deep_learning/blob/main/install/pytorch-install-aug-2023.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "**Manual Python Setup**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software Installation\n",
    "\n",
    "This notebook described how to install PyTorch for GPU (cuda), Apple Metal (MLS), or CPU.\n",
    "\n",
    "## Installing Python and PyTorch\n",
    "\n",
    "It is possible to install and run Python/PyTorch entirely from your computer, without the need for Google CoLab. Running PyTorch locally does require some software configuration and installation.  If you are not confortable with software installation, just use Google CoLab.  These instructions show you how to install PyTorch for CPU, GPU (cuda), and Apple M1/M2/Mx Metal Performance Shaders (MPS). Many of the examples in this class will achieve considerable performance improvement from a GPU/MPS.\n",
    "\n",
    "The first step is to install Python 3.9.  I recommend using the Miniconda (Anaconda) release of Python, as it already includes many of the data science related packages that are needed by this class.  Anaconda directly supports Windows, Mac, and Linux. If you have a Mac and wish to use M1 MPS make sure to install the ARM64 version of Miniconda.  Miniconda is the minimal set of features from the extensive Anaconda Python distribution.  Download Miniconda from the following URL:\n",
    "\n",
    "* [Miniconda](https://docs.conda.io/en/latest/miniconda.html)\n",
    "\n",
    "Make sure that you select the Miniconda version that corrisponds to your operating system. It is particularly important to choose M1/Metal if you have a later (non-Intel) Mac.\n",
    "\n",
    "Once you've installed Miniconda, we will first install Jupyter, which is the editor you will use in this course.\n",
    "\n",
    "```\n",
    "conda install -y jupyter\n",
    "```\n",
    "\n",
    "You must make sure that PyTorch has the version of Python that it is compatible with.  The best way to accomplish this is with an Anaconda environment.  Each environment that you create can have its own Python version, drivers, and Python libraries.  I suggest that you create an environment to hold the Python instance for this class.  Use the following command to create your environment. I am calling the environment **torch**, you can name yours whatever you like. We will create this environment from a YML configuration file. You can obtain this file [here](https://github.com/jeffheaton/app_deep_learning/blob/main/install/torch.yml). You should select from one of the following commands:\n",
    "\n",
    "\n",
    "* **Mac M1/M2**: conda env create -f torch-conda.yml\n",
    "* **NVIDIA CUDA GPU**: conda env create -f torch-cuda.yml\n",
    "* **CPU Only**: conda env create -f torch.yml\n",
    "\n",
    "To enter this environment, you must use the following command: \n",
    "\n",
    "```\n",
    "conda activate torch\n",
    "```\n",
    "\n",
    "\n",
    "## Register your Environment\n",
    "\n",
    "The following command registers your **pytorch** environment. Again, make sure you \"conda activate\" your new **pytorch** environment.\n",
    "\n",
    "```\n",
    "python -m ipykernel install --user --name pytorch --display-name \"Python 3.11 (torch)\"\n",
    "```\n",
    "\n",
    "## Testing your Environment\n",
    "\n",
    "You can now start Jupyter notebook.  Use the following command.\n",
    "\n",
    "```\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "You can now run the following code to check that you have the versions expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Platform: Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\n",
      "PyTorch Version: 2.2.2+cu121\n",
      "\n",
      "Python 3.11.9 (main, Apr 19 2024, 16:48:06) [GCC 11.2.0]\n",
      "Pandas 2.2.2\n",
      "Scikit-Learn 1.4.2\n",
      "NVIDIA/CUDA GPU is available\n",
      "MPS (Apple Metal) is NOT AVAILABLE\n",
      "Target device is cuda\n"
     ]
    }
   ],
   "source": [
    "# What version of Python do you have?\n",
    "import sys\n",
    "import platform\n",
    "import torch\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "\n",
    "has_gpu = torch.cuda.is_available()\n",
    "has_mps = torch.backends.mps.is_built()\n",
    "device = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Python Platform: {platform.platform()}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "print(\"NVIDIA/CUDA GPU is\", \"available\" if has_gpu else \"NOT AVAILABLE\")\n",
    "print(\"MPS (Apple Metal) is\", \"AVAILABLE\" if has_mps else \"NOT AVAILABLE\")\n",
    "print(f\"Target device is {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on MPS Incompatibilities\n",
    "\n",
    "## MPS Warnings\n",
    "\n",
    "You might get MPS warnings, such as the following.\n",
    "\n",
    "```\n",
    "/Users/jeff/miniconda3/envs/torch/lib/python3.9/site-packages/torch/autograd/function.py:539: UserWarning: The operator 'aten::native_dropout' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1702400227158/work/aten/src/ATen/mps/MPSFallback.mm:13.)\n",
    "  return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
    "/Users/jeff/miniconda3/envs/torch/lib/python3.9/site-packages/torch/autograd/__init__.py:394: UserWarning: Error detected in LinearBackward0. Traceback of forward call that caused the error:\n",
    "  File \"/Users/jeff/miniconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
    "    input = module(input)\n",
    " (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1702400227158/work/torch/csrc/autograd/python_anomaly_mode.cpp:119.)\n",
    "  result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
    "[2024-01-07 07:09:59,805] [0/2] torch._dynamo.exc: [WARNING] Backend compiler failed with a fake tensor exception at \n",
    "[2024-01-07 07:09:59,805] [0/2] torch._dynamo.exc: [WARNING]   File \"/Users/jeff/miniconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/container.py\", line 216, in forward\n",
    "[2024-01-07 07:09:59,805] [0/2] torch._dynamo.exc: [WARNING]     return input\n",
    "[2024-01-07 07:09:59,805] [0/2] torch._dynamo.exc: [WARNING] Adding a graph break.\n",
    "/Users/jeff/miniconda3/envs/torch/lib/python3.9/site-packages/torch/autograd/__init__.py:394: UserWarning: Error detected in LinearBackward0. Traceback of forward call that caused the error:\n",
    "  File \"/Users/jeff/miniconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
    "    input = module(input)\n",
    " (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1702400227158/work/torch/csrc/autograd/python_anomaly_mode.cpp:119.)\n",
    "  result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
    "[2024-01-07 07:09:59,846] [0/2] torch._dynamo.exc: [WARNING] Backend compiler failed with a fake tensor exception at \n",
    "[2024-01-07 07:09:59,846] [0/2] torch._dynamo.exc: [WARNING]   File \"/Users/jeff/miniconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/container.py\", line 216, in forward\n",
    "[2024-01-07 07:09:59,846] [0/2] torch._dynamo.exc: [WARNING]     return input\n",
    "[2024-01-07 07:09:59,846] [0/2] torch._dynamo.exc: [WARNING] Adding a graph break.\n",
    "```\n",
    "\n",
    "These warnings are mostly (I believe) harmless; however, you can usually remove them by modifying code like this:\n",
    "\n",
    "```\n",
    "# PyTorch 2.0 Model Compile (improved performance), but does not work as well on MPS\n",
    "#model = torch.compile(model,backend=\"aot_eager\").to(device)\n",
    "model = model.to(device)\n",
    "```\n",
    "\n",
    "## NotImplementedError\n",
    "\n",
    "You will sometimes get a NotImplementedError, this just means that you are trying to use a portion of PyTorch that has not yet enabled MPS. \n",
    "\n",
    "```\n",
    "---------------------------------------------------------------------------\n",
    "NotImplementedError                       Traceback (most recent call last)\n",
    "Cell In[8], line 8\n",
    "      4 temp_device = device\n",
    "      5 #if device == \"mps\":\n",
    "      6 #    device = \"cpu\"\n",
    "----> 8 counts = mandelbrot(\n",
    "      9     # render_size=(1920,1080), # HD\n",
    "     10     render_size=(640, 480),\n",
    "     11     center=(-0.5, 0),\n",
    "     12     zoom=4,\n",
    "     13     cycles=200,\n",
    "     14 )\n",
    "     16 img = render(counts)\n",
    "     17 print(img.size)\n",
    "\n",
    "Cell In[7], line 52, in mandelbrot(render_size, center, zoom, cycles)\n",
    "     48 imag_range = torch.arange(\n",
    "     49     imag_start, imag_end, f, dtype=torch.float32, device=device\n",
    "     50 )\n",
    "     51 real, imag = torch.meshgrid(real_range, imag_range, indexing=\"ij\")\n",
    "---> 52 grid_c = torch.complex(imag, real)\n",
    "     53 current_values = torch.clone(grid_c)\n",
    "     54 counts = torch.Tensor(torch.zeros_like(grid_c, dtype=torch.float32))\n",
    "\n",
    "NotImplementedError: The operator 'aten::complex.out' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.\n",
    "```\n",
    "\n",
    "You can sometimes fix this by adding these lines at the top of your code:\n",
    "\n",
    "```\n",
    "import os\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "```\n",
    "\n",
    "This will not always work, as not all PyTorch code honors this setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
